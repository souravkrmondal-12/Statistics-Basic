# -*- coding: utf-8 -*-
"""Statistics Basic .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wvOG9hmME8qwQKg2zx1Iui7gFfpts6Fr

#Q1. What is statistics, and why is it important?

----
Statistics is a branch of mathematics that deals with collecting, analyzing, interpreting, presenting, and organizing data. It's important for a few key reasons:

▶Understanding Data:
Statistics provides tools and methods to make sense of large and complex datasets, revealing patterns, trends, and insights that might not be obvious otherwise.

▶Informed Decision Making: By analyzing data statistically, we can make more informed and data-driven decisions in various fields, such as business, healthcare, government, and research.

▶Drawing Conclusions: Statistics allows us to draw valid conclusions and make generalizations about a population based on a sample of data, while also quantifying the uncertainty associated with those conclusions.

▶Identifying Relationships: Statistical techniques can help us identify relationships between variables, understand cause and effect, and build predictive models.

▶Evaluating Policies and Programs: Statistics is crucial for evaluating the effectiveness of policies, programs, and interventions in areas like public health, education, and social sciences.

▶Scientific Research: In scientific research, statistics is essential for designing experiments, analyzing data, and interpreting results to validate hypotheses and contribute to knowledge.

statistics empowers us to transform raw data into meaningful information, enabling us to understand the world around us better and make more effective decisions.

#Q2.What are the two main types of statistics ?

----

The two main types of statistics are:

▶Descriptive Statistics: This type of statistics focuses on summarizing and describing the main features of a dataset. It involves methods for organizing, presenting, and summarizing data in a meaningful way. Examples include calculating measures of central tendency (like mean, median, and mode), measures of dispersion (like range, variance, and standard deviation), and creating visualizations (like histograms, bar charts, and pie charts). Descriptive statistics help us get a clear picture of the data we have.

▶Inferential Statistics: This type of statistics goes beyond simply describing the data. It involves using sample data to make inferences, predictions, or generalizations about a larger population from which the sample was drawn. Inferential statistics uses probability theory to draw conclusions and quantify the uncertainty of those conclusions. Examples include hypothesis testing, confidence intervals, and regression analysis. Inferential statistics helps us make educated guesses and draw conclusions about a population based on the information we have from a sample.


Descriptive statistics is about describing the data you have, while inferential statistics is about making inferences about a larger group based on that data.

#Q3.What are descriptive statistics?

---

Descriptive statistics are methods used to summarize and describe the main features of a dataset. They provide simple summaries about the sample and the observations that have been made. These summaries can be either quantitative (like calculating averages or percentages) or visual (like creating charts or graphs).

The main goal of descriptive statistics is to present data in a way that is easy to understand and interpret, highlighting key characteristics such as:

Measures of Central Tendency: These describe the center of the data, such as the mean (average), median (middle value), and mode (most frequent value).

▶Measures of Dispersion (or Variability): These describe the spread or variability of the data, such as the range, variance, and standard deviation.

▶Frequency Distributions: These show how often each value or range of values appears in the dataset.

▶Visualizations: Graphs and charts like histograms, bar charts, pie charts, and box plots are used to visually represent the data's distribution and key features.

Descriptive statistics help you to make sense of your data by organizing and summarizing it so you can see what is happening. They are the first step in most data analysis processes.

#Q4. What is inferential statistics?

----

Inferential statistics is a branch of statistics that uses data from a sample to make inferences or predictions about a larger population. Unlike descriptive statistics, which only describes the data you have, inferential statistics allows you to make generalizations beyond your immediate data.

The main purpose of inferential statistics is to:

▶Estimate Population Parameters: Use sample statistics (like the sample mean or proportion) to estimate unknown population parameters (like the population mean or proportion).

▶Test Hypotheses: Use sample data to test hypotheses about a population. This involves determining whether there is enough evidence to support a claim about the population.

▶Determine Relationships Between Variables: Analyze relationships between variables in a sample to see if those relationships are likely to exist in the larger population.

▶Make Predictions: Build models based on sample data to predict future outcomes or values in the population.


Key concepts in inferential statistics include:

▶Probability: The likelihood of an event occurring, which is fundamental to making inferences and assessing uncertainty.

▶Sampling Distributions: The distribution of a statistic (like the mean) calculated from many different samples of the same size from a population.

▶Hypothesis Testing: A formal procedure for deciding whether to accept or reject a statistical hypothesis.

▶Confidence Intervals: A range of values that is likely to contain an unknown population parameter.

Inferential statistics allows you to draw conclusions and make decisions about a population based on data collected from a sample, taking into account the inherent variability and uncertainty in the data.

#Q5. What is sampling in statistics ?

----
Sampling is the process of selecting a subset of individuals or observations from a larger group (called the population) to collect data and make inferences about the entire population.

It's often impossible or impractical to collect data from every single member of a population, especially if the population is very large. Sampling allows statisticians and researchers to study a smaller, manageable group that is representative of the larger population. By studying the sample, they can draw conclusions and make estimates about the characteristics of the entire population without having to examine every single member.

The key idea behind sampling is to choose a sample that is representative of the population. If the sample is representative, the findings from the sample can be generalized to the population with a certain degree of confidence.

There are different methods of sampling, each with its own advantages and disadvantages. Some common sampling methods include:

▶Random Sampling: Every member of the population has an equal chance of being selected. This helps to minimize bias and increase the likelihood that the sample is representative.

▶Stratified Sampling: The population is divided into subgroups (strata) based on certain characteristics (e.g., age, gender, income), and then a random sample is taken from each subgroup.

▶Cluster Sampling: The population is divided into clusters (e.g., geographic areas), and then a random sample of clusters is selected. All members within the selected clusters are included in the sample.

▶Systematic Sampling: Every nth member of the population is selected after a random starting point.

Choosing the appropriate sampling method is crucial for ensuring that the results of a statistical analysis are valid and can be generalized to the population of interest.

#Q6. What are the different types of sampling methods?

---
There are several different types of sampling methods, each with its own approach to selecting individuals from a population for a study. The choice of sampling method depends on the research question, the nature of the population, and available resources. Here are some of the most common types:

Probability Sampling Methods: These methods involve random selection, ensuring that every member of the population has a known, non-zero probability of being included in the sample. This helps to reduce bias and allows for the use of inferential statistics to generalize findings to the population.

▶ Simple Random Sampling: In this method, every possible sample of a given size from the population has an equal chance of being selected. This can be done by using random number generators or drawing names from a hat. It's straightforward but can be difficult to implement with very large populations.

▶Systematic Sampling: This involves selecting individuals from a list or ordered population at a regular interval. After a random starting point is chosen, every nth individual is selected. This is simpler than simple random sampling but requires a complete list of the population.

▶Stratified Sampling: The population is divided into mutually exclusive subgroups (strata) based on relevant characteristics (e.g., age, gender, income, region). Then, a random sample is drawn from each stratum. This ensures that subgroups are represented proportionally in the sample, which can be important if you want to compare results across groups.

▶Cluster Sampling: The population is divided into clusters (usually geographically based). A random sample of clusters is selected, and then all individuals within the selected clusters are included in the sample. This is useful when the population is spread over a large area, but it can lead to higher sampling error if the clusters are not representative of the population.

Non-Probability Sampling Methods: These methods do not involve random selection, meaning that the probability of a member of the population being selected is unknown. While easier and less expensive to implement, these methods are more prone to bias, and the findings cannot be reliably generalized to the entire population. They are often used in exploratory research or when probability sampling is not feasible.

▶ Convenience Sampling: This involves selecting individuals who are easily accessible or convenient to the researcher. This is the simplest method but is highly susceptible to bias as the sample may not be representative of the population.

▶Voluntary Response Sampling: Individuals choose to participate in the study on their own (e.g., online polls, call-in surveys). This method is also highly biased as individuals with strong opinions are more likely to participate.

▶Purposive (or Judgmental) Sampling: The researcher uses their judgment to select individuals who they believe are most relevant to the research question. This method is subjective and depends on the researcher's expertise.

▶Snowball Sampling: Participants are asked to refer other potential participants who meet the study criteria. This is useful for reaching hidden or hard-to-access populations, but the sample may not be representative.

#Q7. What is the difference between random and non-random sampling?

--
The fundamental difference between random and non-random sampling lies in the method of selection and the implications for generalizing findings to the population.

Here's a breakdown of the key distinctions:

Random Sampling (Probability Sampling):

*Method of Selection: In random sampling, every individual or element in the population has a known, non-zero probability of being selected for the sample. The selection process is based on chance and is free from the researcher's personal bias.

*Representativeness: The goal of random sampling is to obtain a sample that is representative of the population. Because the selection is random, the sample is more likely to have characteristics similar to those of the larger population.

*Generalizability: Findings from a random sample can be generalized to the entire population with a certain level of confidence. Statistical methods (inferential statistics) can be used to estimate population parameters and test hypotheses based on the sample data, and the margin of error can be calculated.

*Bias: Random sampling methods aim to minimize sampling bias, which occurs when the sample is not representative of the population.


Non-Random Sampling (Non-Probability Sampling):

*Method of Selection: In non-random sampling, the selection of individuals is not based on chance. The researcher uses their judgment, convenience, or other non-random criteria to select participants. The probability of any given individual being selected is unknown.

*Representativeness: Non-random samples are less likely to be representative of the population. The selection process can introduce bias, as certain individuals or groups may be more likely to be included than others.

*Generalizability: Findings from a non-random sample cannot be reliably generalized to the entire population. While the findings may be useful for exploring a phenomenon or generating hypotheses, you cannot confidently make inferences about the larger population based on the sample alone.
Bias: Non-random sampling methods are more prone to sampling bias because the selection process is not random.


*Random sampling is like drawing names out of a hat. Everyone has a fair chance of being chosen, and the sample is likely to reflect the larger group.

*Non-random sampling is like picking people for a team based on who is standing closest to you. The selection is not fair or representative, and the team may not reflect the skills of everyone available.

#Q8. Define and give examples of qualitative and quantitative data ?
---

Qualitative data describes qualities or characteristics that are not numerical, while quantitative data is numerical and can be measured or counted. Examples of qualitative data include color, texture, and taste, while examples of quantitative data include height, weight, and temperature.
Qualitative Data:

• Definition: Qualitative data focuses on descriptive variables that cannot be measured or assigned a numeric value.

• Examples:

	• Color of a car (e.g., red, blue, green).   
	• Types of fruits (e.g., apple, banana, orange).
	• Taste of food (e.g., sweet, sour, salty).  
	• Feelings or emotions (e.g., happy, sad, angry).  
	• Open-ended survey responses.
	• Interview transcripts.

Quantitative Data:

• Definition: Quantitative data is numerical data that can be measured or counted.

• Examples:

	• Height of a person (e.g., 5'7").
	• Weight of an object (e.g., 2 kilograms).
	• Temperature of a room (e.g., 22°C).
	• Age of an individual (e.g., 30 years).
	• Number of people in a group.
	• Test scores.
	• Website traffic.

#Q9. What are the different types of data in statistics?

---

In statistics, data is broadly classified into qualitative (categorical) and quantitative (numerical) data. Qualitative data describes qualities or attributes, while quantitative data represents quantities or can be measured numerically.
Qualitative Data:

• Nominal Data: Categories without a natural order or ranking (e.g., colors, gender, types of fruits).
• Ordinal Data: Categories with a meaningful order or ranking, but the intervals between them may not be equal (e.g., customer satisfaction ratings: low, medium, high; educational levels: high school, bachelor's, master's).


Quantitative Data:  

• Discrete Data: Values that can only be counted in whole numbers (e.g., number of students in a class, number of cars sold).
• Continuous Data: Values that can take any value within a range (e.g., height, weight, temperature).

Within quantitative data, there are further classifications based on the level of measurement:

• Interval Data: Values with equal intervals, but no true zero point (e.g., temperature in Celsius or Fahrenheit).
• Ratio Data: Values with equal intervals and a true zero point, allowing for meaningful ratios (e.g., height, weight, income).

#Q10. Explain nominal, ordinal, interval, and ratio levels of measurement.

---
1. Nominal Level
Definition: This is the most basic level. It involves categorizing or labeling data without any order or ranking.

Key Points:

Data are just names or labels.

No meaningful order or ranking.

No mathematical operations are possible (other than counting frequencies).

Examples:

Types of fruit (apple, banana, cherry)

Marital status (single, married, divorced)

Gender (male, female, other)

2. Ordinal Level
Definition: This level involves categorizing data with a meaningful order or rank, but differences between categories aren’t consistent or measurable.

Key Points:

Order matters, but distances between ranks are not equal.

You can compare but not precisely measure differences.

Examples:

Class rankings (1st place, 2nd place, 3rd place)

Survey responses (strongly agree, agree, neutral, disagree, strongly disagree)

Educational level (elementary, high school, college)

3. Interval Level
Definition: Data can be ordered, and differences between values are meaningful and consistent, but there is no true zero point.

Key Points:

Order and exact differences are meaningful.

No true zero—zero does not represent the absence of what’s being measured.

Mathematical operations like addition and subtraction make sense, but ratios do not.

Examples:

Temperature in Celsius or Fahrenheit (0 degrees is arbitrary and not the absence of temperature)

IQ scores

Calendar years (e.g., 1990, 2000, 2010)

4. Ratio Level
Definition: The highest level, with meaningful order, equal intervals, and a true zero point, allowing for the full range of mathematical operations.

Key Points:

Order, meaningful differences, and a true zero.

Ratios make sense—you can say one value is “twice as much” as another.

Examples:

Height, weight, and age (0 means none)

Income (zero dollars means no income)

Distance traveled (0 km means no distance)

| Level    | Order? | Equal Intervals? | True Zero? | Examples                              |
| -------- | ------ | ---------------- | ---------- | ------------------------------------- |
| Nominal  | No     | No               | No         | Fruit types, gender, marital status   |
| Ordinal  | Yes    | No               | No         | Class ranks, survey responses         |
| Interval | Yes    | Yes              | No         | Temperature (°C, °F), IQ scores       |
| Ratio    | Yes    | Yes              | Yes        | Height, weight, age, income, distance |

#Q11. What is the measure of central tendency?

---
A measure of central tendency is a statistical value that represents the center or typical value of a dataset. It provides a summary of the data by identifying the central position within that set of data. The most common measures of central tendency are the mean, median, and mode.

1. Mean (Arithmetic Average)
Definition: The mean is calculated by adding all the values in a dataset and dividing by the number of values.

Formula: Mean = (Sum of all values) / (Number of values)

Example: For the dataset [2, 4, 6, 8, 10], the mean is (2+4+6+8+10)/5 = 30/5 = 6.
Pros: Takes all data points into account; useful for further statistical analysis.
Cons: Sensitive to outliers; extreme values can skew the mean.

2. Median
Definition: The median is the middle value when a dataset is ordered from smallest to largest. If there's an even number of observations, it's the average of the two middle numbers.

Example:

Odd number of values: [3, 5, 7] → Median is 5.

Even number of values: [3, 5, 7, 9] → Median is (5+7)/2 = 6.

Pros: Not affected by outliers; better represents the center in skewed distributions.


3. Mode
Definition: The mode is the value that appears most frequently in a dataset. A dataset can have more than one mode (bimodal or multimodal) or no mode at all.

Example: In [2, 3, 3, 5, 7], the mode is 3.

Pros: Useful for categorical data; identifies the most common value.

Cons: May not reflect the center of the data; not always unique or existent.

Choosing the Appropriate Measure

Symmetrical Distributions: Mean, median, and mode are equal; any can be used.
Skewed Distributions: Median is preferred as it is not affected by extreme values.
Categorical Data: Mode is the only applicable measure.

#Q12. Define mean, median, and mode.

---
Mean is the average of a set of numbers, calculated by summing all values and dividing by the total count. Median is the middle value in a sorted dataset, with half the values above and half below. Mode is the most frequent value in a dataset.

Elaboration:

• Mean (Average): It's the sum of all values in a dataset divided by the number of values. For example, if you have the numbers 2, 4, 6, and 8, the mean is (2+4+6+8)/4 = 5

• Median: To find the median, you first need to arrange the data in ascending order. Then, if the number of values is odd, the median is the middle value. If the number is even, the median is the average of the two middle values. For the dataset 2, 4, 6, 8, the median is (4+6)/2 = 5.

• Mode: The mode is the value that appears most often in the dataset. In the dataset 1, 2, 2, 3, 4, 4, 4, 5, the mode is 4 because it occurs three times, more than any other value.

#Q13.What is the significance of the measure of central tendency?

-----

Measures of central tendency—mean, median, and mode—are fundamental statistical tools that summarize a dataset by identifying its central point. Their significance spans various domains due to the following reasons:


1. Simplification of Complex Data
These measures condense large datasets into a single representative value, facilitating easier interpretation and comparison. For instance, reporting the average income of a region provides a quick insight into its economic status without delving into individual earnings.

2. Informed Decision-Making
In fields like business, education, and healthcare, central tendency measures guide strategic decisions. For example, understanding the average test scores can help educators identify areas needing curriculum adjustments.


3. Identification of Data Distribution Patterns
Analyzing the mean, median, and mode together can reveal the distribution shape of data—whether it's symmetrical, skewed, or has outliers. This understanding is crucial for selecting appropriate statistical methods and models.

4. Benchmarking and Standard Setting
Organizations use these measures to set performance benchmarks. For instance, a company might set sales targets based on the average sales figures from previous quarters.

5. Foundation for Advanced Statistical Analysis
Central tendency measures are foundational for more complex statistical analyses, including variance, standard deviation, and hypothesis testing. They provide the baseline from which deviations and patterns are studied.

#Q14.What is variance, and how is it calculated?

---

Variance is a measure of how spread out or dispersed a set of data is from its average (mean). It's calculated by finding the mean, then determining the squared difference between each data point and the mean, and finally averaging these squared differences. There are two types of variance: population variance (using the entire population) and sample variance (using a subset of the population).

The calculation of variance :

1. Find the Mean: Calculate the average of all data points in the set.
2. Calculate Differences: For each data point, subtract the mean from the data point.
3. Square the Differences: Square each of the differences calculated in step 2.
4. Calculate the Mean of Squared Differences: Average the squared differences.

Formulas:

• Population Variance (σ²):
	• σ² = (Σ(xi - μ)²)/N
		• Where:
			• σ² is the population variance
			• Σ (sigma) indicates the sum of all values
			• xi is each data point in the population
			• μ (mu) is the population mean
			• N is the number of data points in the population

• Sample Variance (s²):
	• s² = (Σ(xi - ¯x)²)/(n-1)
		• Where:
			• s² is the sample variance
			• Σ (sigma) indicates the sum of all values
			• xi is each data point in the sample
			• ¯x (x-bar) is the sample mean
			• n is the number of data points in the sample

#Q15.What is standard deviation, and why is it important?

----

Standard deviation is a statistical measure that indicates the amount of variation or dispersion within a set of data points. It essentially tells you how spread out the values are from their average (mean). A larger standard deviation means the data points are more spread out, while a smaller standard deviation means they are clustered closer to the mean.  
Why is it important?

• Understanding Variability: Standard deviation helps quantify the variability or dispersion in a dataset, providing insights into how much individual data points deviate from the average.

• Identifying Outliers: It helps identify data points that are significantly different from the average, which can be important for detecting errors or unusual values.

• Comparing Datasets: Standard deviation allows for the comparison of datasets to determine if they have similar or different levels of variation.
• Making Informed Decisions: By understanding the variability in a dataset, informed decisions can be made, especially in fields like finance, where it's used to assess risk.  

• Applications in Various Fields: Standard deviation is used in diverse fields like finance (to assess risk), quality control (to ensure consistency), and project management (to track progress and manage risks).

#Q16. Define and explain the term range in statistics?

-----
In statistics, the range is the difference between the highest and lowest values in a dataset. It's a measure of variability, indicating the spread of the data. A large range suggests a wide spread of data points, while a small range indicates the data points are clustered together.
Calculation:

1. Identify the highest value (maximum) in the dataset.
2. Identify the lowest value (minimum) in the dataset.
3. Subtract the minimum value from the maximum value.
	• Range = Maximum Value - Minimum Value  

Example:

• If a dataset is {2, 4, 6, 8, 12}, the range is 12 - 2 = 10.
• If a dataset is {10, 15, 20, 25}, the range is 25 - 10 = 15.

#Q17.  What is the difference between variance and standard deviation?

---Variance and standard deviation are both measures of data spread, but they differ in how they're calculated and what they represent. Variance is the average of the squared differences from the mean, while standard deviation is the square root of the variance. Standard deviation is expressed in the same units as the original data, making it easier to interpret, while variance is in squared units.


• Variance:
	• It's a measure of how much data points vary from the mean.
	• It's calculated by finding the average of the squared differences between each data point and the mean.  
	• The squaring process amplifies the impact of data points that are far from the mean.
	• Variance is often represented as σ², where σ is the standard deviation.
	• Variance is expressed in squared units.

• Standard Deviation:
	• It's the square root of the variance.
	• It represents the typical or average deviation of data points from the mean.
	• Standard deviation is expressed in the same units as the original data.
	• A smaller standard deviation indicates that data points are clustered tightly around the mean, while a larger standard deviation means they are more spread out.

Key Differences Summarized:

| Feature | Variance | Standard Deviation  |
| --- | --- | --- |
| Calculation | Average of squared differences from the mean | Square root of the variance  |
| Units | Squared units (e.g., meters squared) | Same units as the original data (e.g., meters)  |
| Interpretation | Measures the spread of data | Measures the typical distance from the mean  |

#Q18. What is skewness in a dataset ?

----

Skewness in a dataset refers to the asymmetry or lack of symmetry in the distribution of data points around the mean. It describes how the data is spread out, with a skewed distribution having a longer tail on one side compared to the other. A normal distribution, or bell curve, is symmetrical, with the mean, median, and mode all being equal. Skewness indicates how much a distribution deviates from this symmetry.

Types of Skewness:

• Positive Skewness (Right Skew): The tail on the right side of the distribution is longer and fatter than the left side. In this case, the mean is greater than the median, which is often greater than the mode.

• Negative Skewness (Left Skew): The tail on the left side of the distribution is longer and fatter than the right side. The mean is less than the median, which is often less than the mode.

Understanding Skewness:

• Outliers: Skewness can be influenced by outliers, which are extreme values that are far from the majority of the data points.

• Mean vs. Median: Skewness often leads to a difference between the mean and median. The mean is more sensitive to outliers, while the median is more resistant.

• Data Transformations: Skewness can be a problem in some analyses, so data transformations like taking the square root or logarithm can be used to reduce or eliminate skewness.

#Q19.  What does it mean if a dataset is positively or negatively skewed?

---

In a dataset, positive skewness means the distribution is skewed to the right, with the longer tail extending towards higher values. This indicates that most data points cluster around the lower end of the range, and a few extreme high values pull the mean higher than the median. Negative skewness, on the other hand, means the distribution is skewed to the left, with the longer tail extending towards lower values. Here, most data points cluster around the higher end of the range, and a few extreme low values pull the mean lower than the median.
Elaboration:

• Positively Skewed: Imagine a graph where most of the data points are concentrated towards the left side (lower values), but a few very large values extend the graph's tail to the right. This "tail" on the right is the characteristic of positive skewness. In this case, the mean will be larger than the median.

• Negatively Skewed: Conversely, a negatively skewed graph has most of its data points clustered towards the right (higher values), with a few very small values extending the tail to the left. The "tail" on the left is the defining feature of negative skewness. Here, the mean will be smaller than the median.  

Key Differences:

| Feature | Positive Skewness | Negative Skewness  |
| --- | --- | --- |
| Shape | Right-skewed | Left-skewed  |
| Longer Tail | Right | Left  |
| Data Cluster | Lower values | Higher values  |
| Mean vs. Median | Mean &gt; Median | Mean &lt; Median  |

#Q20. Define and explain kurtosis ?

---
Kurtosis is a statistical measure that describes the shape of a distribution's tails in relation to its overall shape. Specifically, it quantifies the "tailedness" of the probability distribution of a real-valued random variable. Unlike measures that assess central tendency or variability, kurtosis focuses on the extremities of the data distribution.


📉 Types of Kurtosis
Kurtosis is categorized into three types based on its value:

Mesokurtic (Kurtosis ≈ 3):

Represents a normal distribution.

Tails are neither heavy nor light; data is moderately concentrated around the mean.

Leptokurtic (Kurtosis > 3):

Distributions have heavy tails and a sharp peak.

Indicates a higher probability of extreme values (outliers).

Common in financial data, where extreme events occur more frequently than predicted by a normal distribution.
investopedia.com

Platykurtic (Kurtosis < 3):

Distributions have light tails and a flatter peak.

Indicates fewer extreme values than a normal distribution.

#Q21. What is the purpose of covariance?

----

Covariance primarily serves to measure the directional relationship between two variables. It indicates whether two variables tend to move in the same direction (positive covariance) or in opposite directions (negative covariance), according to a math website. In essence, it helps understand how much two variables change together.
Here's a more detailed look:

• Directional Relationship: Covariance tells you if a variable's increase is associated with an increase or decrease in another variable, according to a website.

• Joint Variability: It quantifies how much two variables change together, providing insights into their joint variability.

• Applications:

• Finance: Investopedia uses covariance to understand the relationship between the returns of different assets in a portfolio, which is crucial for portfolio diversification.  

• Statistics: Covariance is used in statistical analysis to understand how two variables relate to each other.

• Data Analysis: Covariance can be used in data preprocessing techniques like principal component analysis to reduce dimensionality.

#Q22. What does correlation measure in statistics ?

-----

In statistics, correlation measures the strength and direction of the relationship between two variables. It quantifies how much one variable tends to change in relation to changes in another. Correlation does not imply causation; it simply indicates an association between variables.

Key aspects of correlation:

• Strength: The correlation coefficient (typically denoted as "r") ranges from -1 to +1. A value of 1 indicates a perfect positive linear relationship, -1 a perfect negative linear relationship, and 0 no linear relationship.

• Direction: The sign of the correlation coefficient indicates the direction of the relationship. A positive correlation means the variables move in the same direction (e.g., as one increases, the other tends to increase), while a negative correlation means they move in opposite directions (e.g., as one increases, the other tends to decrease).

• Linearity: Correlation is most useful when there is a linear relationship between the variables, meaning the relationship can be reasonably represented by a straight line.

• Not Causation: Correlation does not mean that one variable causes the other. There may be other factors (confounding variables) influencing both variables, or a third variable could be the cause of both.


Examples:

• Positive Correlation: Height and weight, income and education level, advertising spending and sales.
• Negative Correlation: Temperature and ice cream sales, hours of study and exam grade (with some exceptions).
• No Correlation: Shoe size and intelligence.

#Q23. What is the difference between covariance and correlation ?


----

Covariance measures how two random variables change together, while correlation measures the strength and direction of their linear relationship. Covariance can range from negative to positive infinity, while correlation is always between -1 and 1. Correlation is essentially a standardized version of covariance, making it easier to compare relationships across different variables. [1, 2, 3, 4, 5]  
Here's a more detailed breakdown:
Covariance:

• Definition: A measure of how much two random variables change together. A positive covariance indicates that the variables tend to move in the same direction (when one increases, the other tends to increase), while a negative covariance indicates that they tend to move in opposite directions.

• Range: Covariance can take any value between negative infinity and positive infinity.

• Interpretation: Covariance is sensitive to the scale of the variables. A larger magnitude doesn't necessarily mean a stronger relationship.

Correlation:

• Definition: A statistical measure that indicates how strongly two variables are related, typically linearly.

• Range: Correlation coefficients are always between -1 and 1.

• Interpretation:
	• A value of -1 or 1 indicates a strong linear relationship (either positive or negative).
	• A value of 0 indicates no linear relationship.
	• The sign of the correlation coefficient indicates the direction of the relationship (positive or negative).

Key Differences:

| Feature | Covariance | Correlation  |
| --- | --- | --- |
| Definition | Measures how much two variables change together | Measures the strength and direction of the linear relationship  |
| Range | -∞ to +∞ | -1 to +1  |
| Interpretation | Sensitive to the scale of the variables | Standardized, easier to compare relationships  |
| Standardization | Not standardized | Standardized (calculated by dividing covariance by the product of the standard deviations)  |

In essence: Covariance tells you the direction of the relationship, while correlation tells you the direction and strength of the linear relationship. Correlation is a more useful measure for assessing the strength of a linear relationship because it is standardized and doesn't depend on the scale of the variables.

#Q24. What are some real-world applications of statistics?

---
Statistics finds practical applications in diverse fields like business, education, government, and sports, impacting areas from market research and sales forecasting to evaluating teaching methods and understanding population trends. It's also crucial in healthcare, weather forecasting, and various scientific disciplines.  

1. Business and Economics:

• Market Research: Companies use statistics to understand customer behavior, market trends, and competitive activity, informing product development and marketing strategies.  

• Sales Forecasting: Statistical methods predict future sales based on historical data, helping businesses plan production and marketing activities.

• Quality Control: Statistical quality control methods identify defects and production problems, helping maintain quality standards.

• Financial Analysis: Statistics analyze financial data to evaluate performance and predict future financial performance.   

• Risk Assessment: Businesses use statistics to assess and control risks by analyzing past data and estimating potential dangers.

• Economic Planning: Statistics inform government decisions on budget planning, public health, and infrastructure by analyzing population trends and economic conditions.   

2. Education and Research:

• Evaluating Teaching Methods: Statistics helps determine the effectiveness of different teaching methods.

• Scientific Research: Statistical analysis is used to interpret and draw conclusions from research data, including clinical trials and epidemiological studies.

• Meta-Analysis: Statistics is used to synthesize findings from multiple studies in a particular area.

• Survey Design: Statistical methods are used to design effective surveys and analyze the results.

• Statistical Modeling: Statistics allows for the development of predictive models in various fields.  

3. Government and Public Policy:

• Population Trends: Statistics helps understand demographic changes and population growth.
• Public Policy: Governments use statistics to assess the effectiveness of public policy and make informed decisions on resource allocation.
• Census Data Analysis: Statistics are used to analyze census data and understand population distribution.
• Urban Development: Statistics helps ensure that resources are distributed fairly and efficiently across regions during urban planning.  

4. Healthcare:

• Disease Prediction: Statistics helps predict the spread and prevalence of diseases.
• Medical Studies: Statistics are used to evaluate the effectiveness of treatments and drugs in clinical trials.

• Genetics: Statistical methods are used to determine the likelihood of individuals inheriting genetic diseases.
• Public Health: Statistics are used to monitor disease outbreaks and implement public health measures.  

5. Other Applications:

• Weather Forecasting: Statistical models use past weather data to predict future weather patterns.

• Sports Analytics: Statistics are used to analyze player and team performance, identify strengths and weaknesses, and predict game outcomes.

• Insurance: Insurance companies use statistics to assess risk, determine premiums, and predict payouts.

• Quality Control: Statistics are used to ensure quality in various manufacturing processes.

• Market Research: Statistics help understand consumer behavior and preferences.

• Financial Analysis: Statistics helps assess the risk and potential of financial investments.

#Practical##

#Q1. How do you calculate the mean, median, and mode of a dataset?
"""

import statistics

def calculate_mean(data):
    """
    Calculates the mean (average) of a dataset.

    Args:
    data: A list or iterable of numerical values.

    Returns:
    The mean of the data.
    """
    return sum(data) / len(data)

def calculate_median(data):
    """
    Calculates the median of a dataset.

    Args:
    data: A list or iterable of numerical values.

    Returns:
    The median of the data.
    """
    sorted_data = sorted(data)
    n = len(sorted_data)
    if n % 2 == 0:  # Even number of elements
        mid1 = sorted_data[n // 2 - 1]
        mid2 = sorted_data[n // 2]
        return (mid1 + mid2) / 2
    else:  # Odd number of elements
        return sorted_data[n // 2]

def calculate_mode(data):
    """
    Calculates the mode (most frequent value) of a dataset.

    Args:
    data: A list or iterable of numerical values.

    Returns:
    A list of modes if multiple values have the same highest frequency,
    or a single mode if only one value is most frequent.
    """
    counts = {}
    for item in data:
        counts[item] = counts.get(item, 0) + 1

    max_count = 0
    modes = []

    for item, count in counts.items():
        if count > max_count:
            max_count = count
            modes = [item]
        elif count == max_count:
            modes.append(item)

    return modes

# Example usage:
data = [1, 2, 2, 3, 3, 3, 4, 5]

mean_value = calculate_mean(data)
median_value = calculate_median(data)
mode_values = calculate_mode(data)

print(f"Mean: {mean_value}")
print(f"Median: {median_value}")
print(f"Mode: {mode_values}")

# Using the statistics module
print(f"Mean (using statistics): {statistics.mean(data)}")
print(f"Median (using statistics): {statistics.median(data)}")
print(f"Mode (using statistics): {statistics.mode(data)}")

"""#Q2.  Write a Python program to compute the variance and standard deviation of a dataset ."""

import math

def calculate_variance(data):
    """Calculates the variance of a dataset.

    Args:
        data: A list of numerical data.

    Returns:
        The variance of the data.
    """
    n = len(data)
    if n < 2:
        raise ValueError("Data must contain at least two elements")
    mean = sum(data) / n
    return sum([(x - mean) ** 2 for x in data]) / (n - 1)

def calculate_standard_deviation(data):
    """Calculates the standard deviation of a dataset.

    Args:
        data: A list of numerical data.

    Returns:
        The standard deviation of the data.
    """
    variance = calculate_variance(data)
    return math.sqrt(variance)

if __name__ == "__main__":
    dataset = [1, 2, 3, 4, 5]
    variance = calculate_variance(dataset)
    standard_deviation = calculate_standard_deviation(dataset)
    print(f"Dataset: {dataset}")
    print(f"Variance: {variance}")
    print(f"Standard Deviation: {standard_deviation}")

"""#Q3. Create a dataset and classify it into nominal, ordinal, interval, and ratio types.


"""

import pandas as pd
from enum import Enum

# Define data types as an Enum for clarity
class DataType(Enum):
    NOMINAL = "nominal"
    ORDINAL = "ordinal"
    INTERVAL = "interval"
    RATIO = "ratio"

# 1. Create a Sample Dataset
data = {
    'name': ['sourav', 'pinki', 'SOurav', 'PInki', 'PiNKI'],  # Nominal
    'gender': ['Female', 'Male', 'Male', 'Male', 'Female'],  # Nominal
    'age': [25, 30, 28, 32, 22],  # Ratio
    'satisfaction': ['Very Satisfied', 'Neutral', 'Satisfied', 'Very Satisfied', 'Dissatisfied'],  # Ordinal
    'temperature': [20, 22, 25, 23, 18],  # Interval
    'income': [50000, 60000, 55000, 70000, 45000],  # Ratio
    'education': ['High School', 'Bachelor', 'Associate', 'Master', 'High School'],  # Nominal
    'location': ['Patna', 'Delhi', 'Mumbai', 'Bangalore', 'Hyderabad'],  # Nominal
}

df = pd.DataFrame(data)

# 2. Define a function to classify data types
def classify_data_type(series):
    """
    Classifies a pandas Series (column) into one of the four data types.
    """
    if series.dtype in ['object', 'string']:
        if series.nunique() <= 5:
            # If few unique values, consider it nominal or ordinal
            if series.unique().tolist() == ['Very Satisfied', 'Satisfied', 'Neutral', 'Dissatisfied', 'Very Dissatisfied']:
                return DataType.ORDINAL
            else:
                return DataType.NOMINAL
        else:
            return DataType.NOMINAL # Assume nominal if many categories
    elif series.dtype in ['int64', 'float64']:
        # If it's numeric, check for true zero and equal intervals
        if 0 in series.unique():
            return DataType.RATIO # Ratio data has a true zero
        else:
            return DataType.INTERVAL # If no true zero, consider it interval

    return None  # Handle unexpected cases


# 3. Classify Each Column
column_types = {}
for column in df.columns:
    column_types[column] = classify_data_type(df[column])

# 4. Display the classifications
print(column_types)

# 5. (Optional) Create a new DataFrame with type classifications
type_df = pd.DataFrame(
    {'column': df.columns, 'data_type': [column_types[col].value for col in df.columns]}
)
print("\nData Type DataFrame:")
print(type_df)

"""#Q4. Implement sampling techniques like random sampling and stratified sampling.


"""

import random

def simple_random_sampling(population, sample_size):
  """
  Performs simple random sampling.

  Args:
    population: A list or array representing the population.
    sample_size: The desired size of the sample.

  Returns:
    A list representing the random sample.
  """
  if sample_size > len(population):
    raise ValueError("Sample size cannot exceed population size.")
  return random.sample(population, sample_size)

def stratified_sampling(population, strata, sample_size_per_stratum):
  """
  Performs stratified sampling.

  Args:
    population: A list of dictionaries, where each dictionary represents a population member
      and has keys for 'group' (stratum) and 'data' (data to be sampled).
    strata: A list of unique stratum names.
    sample_size_per_stratum: A dictionary mapping each stratum to its desired sample size.

  Returns:
    A list of dictionaries, representing the stratified sample.
  """

  stratified_sample = []

  for stratum in strata:
    stratum_population = [member for member in population if member['group'] == stratum]
    if len(stratum_population) < sample_size_per_stratum[stratum]:
      raise ValueError(f"Sample size for stratum {stratum} exceeds the population size.")

    sample = random.sample(stratum_population, sample_size_per_stratum[stratum])
    stratified_sample.extend(sample)

  return stratified_sample

# Example usage:
# Define a population (list of integers) for simple random sampling
population = list(range(1, 101))
sample_size = 20
try:
  sample = simple_random_sampling(population, sample_size)
  print("Simple Random Sample:", sample)
except ValueError as e:
  print("Error:", e)

# Define a population for stratified sampling
population_stratified = [
  {'group': 'A', 'data': 1}, {'group': 'A', 'data': 2}, {'group': 'A', 'data': 3},
  {'group': 'B', 'data': 4}, {'group': 'B', 'data': 5}, {'group': 'B', 'data': 6},
  {'group': 'C', 'data': 7}, {'group': 'C', 'data': 8}, {'group': 'C', 'data': 9}, {'group': 'C', 'data': 10}
]
strata = ['A', 'B', 'C']
sample_size_per_stratum = {'A': 2, 'B': 2, 'C': 3}  # Sample 2 from A, 2 from B, 3 from C

try:
  stratified_sample = stratified_sampling(population_stratified, strata, sample_size_per_stratum)
  print("\nStratified Sample:")
  for member in stratified_sample:
    print(f"Group: {member['group']}, Data: {member['data']}")
except ValueError as e:
  print("Error:", e)

"""#Q5. Write a Python function to calculate the range of a dataset?"""

def calculate_range(data):
    """
    Calculates the range of a dataset.

    Args:
    data (list or tuple): A list or tuple of numerical data.

    Returns:
    float: The range of the dataset (maximum value - minimum value).
           Returns None if the input data is empty.
    """
    if not data:
        return None

    min_val = min(data)
    max_val = max(data)
    data_range = max_val - min_val

    return data_range

data = [10, 5, 20, 8, 15]
range_of_data = calculate_range(data)
print(f"The range of the dataset is: {range_of_data}")

"""#Q6.Create a dataset and plot its histogram to visualize skewness."""



import matplotlib.pyplot as plt
import numpy as np

# Create a dataset with positive skewness (e.g., income data)
# Most values are lower, with a few higher outliers pulling the mean to the right
positive_skewed_data = np.concatenate([np.random.normal(50000, 15000, 900),
                                       np.random.normal(150000, 40000, 100)])


# Create a dataset with negative skewness (e.g., exam scores with a ceiling effect)
# Most values are higher, with a few lower outliers pulling the mean to the left
negative_skewed_data = np.concatenate([np.random.normal(80, 10, 800),
                                       np.random.normal(30, 15, 200)])

# Create a dataset with approximately no skewness (normal distribution)
normal_data = np.random.normal(70, 10, 1000)

datasets = {
    "Positive Skewness": positive_skewed_data,
    "Negative Skewness": negative_skewed_data,
    "No Skewness (Normal)": normal_data
}

# Plot histograms for each dataset
plt.figure(figsize=(15, 5))

for i, (name, data) in enumerate(datasets.items()):
    plt.subplot(1, 3, i + 1)
    plt.hist(data, bins=30, edgecolor='black')
    plt.title(name)
    plt.xlabel("Value")
    plt.ylabel("Frequency")

plt.tight_layout()
plt.show()

"""#Q7. Calculate skewness and kurtosis of a dataset using Python libraries."""

from scipy.stats import skew, kurtosis
import numpy as np

# Assuming 'datasets' dictionary is already defined from the previous step
# If not, you can create or load your dataset here.
# Example:
# datasets = {
#     "My Dataset": np.random.normal(0, 1, 1000) # Example dataset
# }


print("Calculating Skewness and Kurtosis:")
for name, data in datasets.items():
    data_skewness = skew(data)
    data_kurtosis = kurtosis(data) # By default, Fisher's kurtosis (excess kurtosis) is calculated

    print(f"\nDataset: {name}")
    print(f"Skewness: {data_skewness:.4f}")
    print(f"Kurtosis (Fisher): {data_kurtosis:.4f}")

"""#Q8.Generate a dataset and demonstrate positive and negative skewness."""

import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats

# Generate a dataset with positive skewness
# We'll use a gamma distribution to create a positively skewed dataset
n = 1000  # Number of data points
shape = 2  # Shape parameter (affects the skewness)
scale = 2  # Scale parameter
positive_skewed_data = np.random.gamma(shape, scale, n)

# Generate a dataset with negative skewness
# We'll modify the positive skewed data to create negative skew
negative_skewed_data = -positive_skewed_data

# Display histograms to visualize the skewness
plt.figure(figsize=(10, 5))

# Positive skew
plt.subplot(1, 2, 1)
plt.hist(positive_skewed_data, bins=30, alpha=0.7, label='Positive Skew')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.title('Positive Skewed Data')
plt.grid(True)

# Negative skew
plt.subplot(1, 2, 2)
plt.hist(negative_skewed_data, bins=30, alpha=0.7, label='Negative Skew')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.title('Negative Skewed Data')
plt.grid(True)

plt.tight_layout()
plt.show()

# Calculate and print skewness values (optional)
print(f"Skewness of positive data: {stats.skew(positive_skewed_data):.2f}")
print(f"Skewness of negative data: {stats.skew(negative_skewed_data):.2f}")

"""#Q9.Write a Python script to calculate covariance between two datasets."""

def calculate_covariance(data1, data2):
    """
    Calculates the covariance between two datasets.

    Args:
    data1: A list or iterable of numerical values for the first dataset.
    data2: A list or iterable of numerical values for the second dataset.

    Returns:
    The covariance between the two datasets.
    Returns None if the datasets are of different lengths or are empty.
    """
    if len(data1) != len(data2) or not data1:
        return None

    n = len(data1)
    mean1 = sum(data1) / n
    mean2 = sum(data2) / n

    covariance = sum([(x - mean1) * (y - mean2) for x, y in zip(data1, data2)]) / (n - 1)

    return covariance

# Example usage:
dataset1 = [1, 2, 3, 4, 5]
dataset2 = [5, 4, 3, 2, 1] # Negative relationship
dataset3 = [1, 2, 1.5, 3, 2.5] # Positive relationship

cov1_2 = calculate_covariance(dataset1, dataset2)
cov1_3 = calculate_covariance(dataset1, dataset3)

print(f"Covariance between dataset1 and dataset2: {cov1_2}")
print(f"Covariance between dataset1 and dataset3: {cov1_3}")

"""#Q10.Write a Python script to calculate the correlation coefficient between two datasets."""

import numpy as np

def calculate_correlation_coefficient(data1, data2):
    """
    Calculates the Pearson correlation coefficient between two datasets.

    Args:
    data1: A list or iterable of numerical values for the first dataset.
    data2: A list or iterable of numerical values for the second dataset.

    Returns:
    The Pearson correlation coefficient between the two datasets.
    Returns None if the datasets are of different lengths or are empty.
    """
    if len(data1) != len(data2) or not data1:
        return None

    # Calculate the correlation coefficient using numpy's corrcoef function
    correlation_matrix = np.corrcoef(data1, data2)

    # The correlation coefficient between data1 and data2 is the off-diagonal element
    correlation_coefficient = correlation_matrix[0, 1]

    return correlation_coefficient

# Example usage:
dataset1 = [1, 2, 3, 4, 5]
dataset2 = [5, 4, 3, 2, 1] # Negative relationship
dataset3 = [1, 2, 1.5, 3, 2.5] # Positive relationship

corr1_2 = calculate_correlation_coefficient(dataset1, dataset2)
corr1_3 = calculate_correlation_coefficient(dataset1, dataset3)

print(f"Correlation coefficient between dataset1 and dataset2: {corr1_2}")
print(f"Correlation coefficient between dataset1 and dataset3: {corr1_3}")

"""#Q11. Create a scatter plot to visualize the relationship between two variables."""

import matplotlib.pyplot as plt

# Example usage with datasets from the previous step
dataset1 = [1, 2, 3, 4, 5]
dataset2 = [5, 4, 3, 2, 1]  # Negative relationship
dataset3 = [1, 2, 1.5, 3, 2.5]  # Positive relationship

# Create a scatter plot for dataset1 and dataset2
plt.figure(figsize=(8, 6))
plt.scatter(dataset1, dataset2)
plt.title('Scatter Plot of Dataset1 vs Dataset2 (Negative Correlation)')
plt.xlabel('Dataset 1')
plt.ylabel('Dataset 2')
plt.grid(True)
plt.show()

# Create a scatter plot for dataset1 and dataset3
plt.figure(figsize=(8, 6))
plt.scatter(dataset1, dataset3)
plt.title('Scatter Plot of Dataset1 vs Dataset3 (Positive Correlation)')
plt.xlabel('Dataset 1')
plt.ylabel('Dataset 3')
plt.grid(True)
plt.show()

"""#Q12. Implement and compare simple random sampling and systematic sampling."""

import random

def simple_random_sampling(population, sample_size):
  """
  Performs simple random sampling.

  Args:
    population: A list or array representing the population.
    sample_size: The desired size of the sample.

  Returns:
    A list representing the random sample.
  """
  if sample_size > len(population):
    raise ValueError("Sample size cannot exceed population size.")
  return random.sample(population, sample_size)

def systematic_sampling(population, sample_size):
    """
    Performs systematic sampling.

    Args:
        population: A list or array representing the population.
        sample_size: The desired size of the sample.

    Returns:
        A list representing the systematic sample.
    """
    if sample_size > len(population):
        raise ValueError("Sample size cannot exceed population size.")
    step = len(population) // sample_size
    start_index = random.randint(0, step - 1)
    return [population[i] for i in range(start_index, len(population), step)][:sample_size]

# Example Usage:
population = list(range(1, 101)) # A population of numbers from 1 to 100
sample_size = 10

# Simple Random Sampling
random_sample = simple_random_sampling(population, sample_size)
print("Simple Random Sample:", random_sample)

# Systematic Sampling
systematic_sample = systematic_sampling(population, sample_size)
print("Systematic Sample:", systematic_sample)

# To compare, you could calculate statistics (like mean or median) for each sample
# and compare them to the population statistics if known.

"""#Q13.Calculate the mean, median, and mode of grouped data."""

def calculate_grouped_statistics(class_intervals, frequencies):
    # Calculate midpoints
    midpoints = [(interval[0] + interval[1]) / 2 for interval in class_intervals]

    # Total frequency
    total_freq = sum(frequencies)

    # Mean calculation
    mean = sum(f * m for f, m in zip(frequencies, midpoints)) / total_freq

    # Cumulative frequencies
    cumulative_freq = [sum(frequencies[:i+1]) for i in range(len(frequencies))]

    # Median calculation
    N = total_freq
    median_class_index = next(i for i, cf in enumerate(cumulative_freq) if cf >= N / 2)
    l_median = class_intervals[median_class_index][0]
    h = class_intervals[median_class_index][1] - class_intervals[median_class_index][0]
    cf = cumulative_freq[median_class_index - 1] if median_class_index != 0 else 0
    f = frequencies[median_class_index]
    median = l_median + ((N / 2 - cf) / f) * h

    # Mode calculation
    modal_class_index = frequencies.index(max(frequencies))
    l_modal = class_intervals[modal_class_index][0]
    f1 = frequencies[modal_class_index]
    f0 = frequencies[modal_class_index - 1] if modal_class_index != 0 else 0
    f2 = frequencies[modal_class_index + 1] if modal_class_index + 1 < len(frequencies) else 0
    mode = l_modal + ((f1 - f0) / (2 * f1 - f0 - f2)) * h if (2 * f1 - f0 - f2) != 0 else None

    return mean, median, mode

# Example usage
class_intervals = [(10, 20), (20, 30), (30, 40), (40, 50), (50, 60)]
frequencies = [5, 8, 15, 10, 7]

mean, median, mode = calculate_grouped_statistics(class_intervals, frequencies)
print(f"Mean: {mean}")
print(f"Median: {median}")
print(f"Mode: {mode}")

"""#Q14.Simulate data using Python and calculate its central tendency and dispersion."""

import numpy as np
import math

def calculate_mean(data):
    """
    Calculates the mean (average) of a dataset.

    Args:
    data: A list or iterable of numerical values.

    Returns:
    The mean of the data.
    """
    return sum(data) / len(data)

def calculate_median(data):
    """
    Calculates the median of a dataset.

    Args:
    data: A list or iterable of numerical values.

    Returns:
    The median of the data.
    """
    sorted_data = sorted(data)
    n = len(sorted_data)
    if n % 2 == 0:  # Even number of elements
        mid1 = sorted_data[n // 2 - 1]
        mid2 = sorted_data[n // 2]
        return (mid1 + mid2) / 2
    else:  # Odd number of elements
        return sorted_data[n // 2]

def calculate_mode(data):
    """
    Calculates the mode (most frequent value) of a dataset.

    Args:
    data: A list or iterable of numerical values.

    Returns:
    A list of modes if multiple values have the same highest frequency,
    or a single mode if only one value is most frequent.
    """
    counts = {}
    for item in data:
        counts[item] = counts.get(item, 0) + 1

    max_count = 0
    modes = []

    for item, count in counts.items():
        if count > max_count:
            max_count = count
            modes = [item]
        elif count == max_count:
            modes.append(item)

    return modes

def calculate_variance(data):
    """Calculates the variance of a dataset.

    Args:
        data: A list of numerical data.

    Returns:
        The variance of the data.
    """
    n = len(data)
    if n < 2:
        raise ValueError("Data must contain at least two elements")
    mean = sum(data) / n
    return sum([(x - mean) ** 2 for x in data]) / (n - 1)

def calculate_standard_deviation(data):
    """Calculates the standard deviation of a dataset.

    Args:
        data: A list of numerical data.

    Returns:
        The standard deviation of the data.
    """
    variance = calculate_variance(data)
    return math.sqrt(variance)

def calculate_range(data):
    """
    Calculates the range of a dataset.

    Args:
    data (list or tuple): A list or tuple of numerical data.

    Returns:
    float: The range of the dataset (maximum value - minimum value).
           Returns None if the input data is empty.
    """
    if not data:
        return None

    min_val = min(data)
    max_val = max(data)
    data_range = max_val - min_val

    return data_range


# Simulate a dataset (e.g., a normal distribution)
simulated_data = np.random.normal(loc=50, scale=10, size=100) # Mean=50, Standard Deviation=10, 100 data points

# Convert numpy array to a list for compatibility with previous functions
simulated_data_list = simulated_data.tolist()

# Calculate measures of central tendency
mean_simulated = calculate_mean(simulated_data_list)
median_simulated = calculate_median(simulated_data_list)
mode_simulated = calculate_mode(simulated_data_list) # Note: Mode might not be meaningful for continuous data

# Calculate measures of dispersion
variance_simulated = calculate_variance(simulated_data_list)
std_dev_simulated = calculate_standard_deviation(simulated_data_list)
range_simulated = calculate_range(simulated_data_list)


print(f"Simulated Dataset (first 10 elements): {simulated_data_list[:10]}...")
print(f"Mean: {mean_simulated:.2f}")
print(f"Median: {median_simulated:.2f}")
print(f"Mode: {mode_simulated}") # Mode is less informative for continuous data
print(f"Variance: {variance_simulated:.2f}")
print(f"Standard Deviation: {std_dev_simulated:.2f}")
print(f"Range: {range_simulated:.2f}")

"""#Q15.Use NumPy or pandas to summarize a dataset’s descriptive statistics."""

import pandas as pd
import numpy as np

# Create a sample dataset using pandas
data = {
    'Numerical_Column1': np.random.rand(100) * 100,  # Random numbers between 0 and 100
    'Numerical_Column2': np.random.randint(1, 50, 100), # Random integers between 1 and 49
    'Categorical_Column': np.random.choice(['A', 'B', 'C', 'D'], 100) # Random categories
}
df = pd.DataFrame(data)

# Use the .describe() method to get descriptive statistics
descriptive_stats = df.describe()

# Display the descriptive statistics
print("Descriptive Statistics using pandas:")
display(descriptive_stats)

# For non-numerical columns, you can use include='all'
descriptive_stats_all = df.describe(include='all')
print("\nDescriptive Statistics including categorical data:")
display(descriptive_stats_all)

"""#Q16. Plot a boxplot to understand the spread and identify outliers."""

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'df' pandas DataFrame is available from the previous step
# If not, you can create or load your DataFrame here.
# Example:
# data = {'Numerical_Column': [10, 20, 30, 40, 50, 150]}
# df = pd.DataFrame(data)

# Create a boxplot for 'Numerical_Column1'
plt.figure(figsize=(8, 6))
sns.boxplot(x=df['Numerical_Column1'])
plt.title('Boxplot of Numerical_Column1')
plt.xlabel('Numerical_Column1')
plt.show()

# Create a boxplot for 'Numerical_Column2'
plt.figure(figsize=(8, 6))
sns.boxplot(x=df['Numerical_Column2'])
plt.title('Boxplot of Numerical_Column2')
plt.xlabel('Numerical_Column2')
plt.show()

"""#Q17.Calculate the interquartile range (IQR) of a dataset."""

# Assuming 'df' pandas DataFrame is available from previous steps
# If not, you can create or load your DataFrame here.
# Example:
# import pandas as pd
# data = {'Numerical_Column': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]}
# df = pd.DataFrame(data)

# Calculate the first quartile (Q1) and third quartile (Q3)
Q1 = df['Numerical_Column1'].quantile(0.25)
Q3 = df['Numerical_Column1'].quantile(0.75)

# Calculate the Interquartile Range (IQR)
IQR = Q3 - Q1

print(f"First Quartile (Q1) of Numerical_Column1: {Q1:.2f}")
print(f"Third Quartile (Q3) of Numerical_Column1: {Q3:.2f}")
print(f"Interquartile Range (IQR) of Numerical_Column1: {IQR:.2f}")

# You can do the same for 'Numerical_Column2'
Q1_2 = df['Numerical_Column2'].quantile(0.25)
Q3_2 = df['Numerical_Column2'].quantile(0.75)
IQR_2 = Q3_2 - Q1_2

print(f"\nFirst Quartile (Q1) of Numerical_Column2: {Q1_2:.2f}")
print(f"Third Quartile (Q3) of Numerical_Column2: {Q3_2:.2f}")
print(f"Interquartile Range (IQR) of Numerical_Column2: {IQR_2:.2f}")

"""#Q18. Implement Z-score normalization and explain its significance."""

import numpy as np
import pandas as pd

def z_score_normalize(data):
    """
    Performs Z-score normalization on a dataset.

    Args:
    data: A list, numpy array, or pandas Series of numerical values.

    Returns:
    A numpy array containing the Z-score normalized data.
    Returns None if the standard deviation is zero (all values are the same).
    """
    data = np.array(data) # Convert to numpy array for easier calculations
    mean = np.mean(data)
    std_dev = np.std(data)

    if std_dev == 0:
        return None # Avoid division by zero if all values are the same

    z_scores = (data - mean) / std_dev

    return z_scores

# Example usage with a sample dataset
dataset = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]
z_normalized_data = z_score_normalize(dataset)

print("Original Dataset:", dataset)
print("Z-score Normalized Dataset:", z_normalized_data)

# Example with a pandas Series from the existing DataFrame
if 'df' in locals() and 'Numerical_Column1' in df.columns:
    z_normalized_series = z_score_normalize(df['Numerical_Column1'])
    print("\nOriginal Numerical_Column1 (first 10):", df['Numerical_Column1'].head(10).tolist())
    print("Z-score Normalized Numerical_Column1 (first 10):", z_normalized_series[:10])

"""#Q19. Compare two datasets using their standard deviations."""

# Assuming 'df' pandas DataFrame is available from previous steps
# If not, you can create or load your DataFrame here.
# Example:
# import pandas as pd
# import numpy as np
# data = {'Dataset1': np.random.normal(50, 10, 100),
#         'Dataset2': np.random.normal(50, 20, 100)}
# df = pd.DataFrame(data)

# Get the standard deviations from the descriptive statistics
# If you don't have the descriptive_stats DataFrame, you can calculate them directly:
# std_dev_dataset1 = df['Numerical_Column1'].std()
# std_dev_dataset2 = df['Numerical_Column2'].std()

# Using the descriptive_stats DataFrame from the previous step
if 'descriptive_stats' in locals():
    std_dev_dataset1 = descriptive_stats.loc['std', 'Numerical_Column1']
    std_dev_dataset2 = descriptive_stats.loc['std', 'Numerical_Column2']

    print(f"Standard Deviation of Numerical_Column1: {std_dev_dataset1:.2f}")
    print(f"Standard Deviation of Numerical_Column2: {std_dev_dataset2:.2f}")

    # Compare the standard deviations
    if std_dev_dataset1 > std_dev_dataset2:
        print("\nNumerical_Column1 has a larger standard deviation than Numerical_Column2.")
        print("This means the data points in Numerical_Column1 are more spread out from the mean compared to Numerical_Column2.")
    elif std_dev_dataset1 < std_dev_dataset2:
        print("\nNumerical_Column2 has a larger standard deviation than Numerical_Column1.")
        print("This means the data points in Numerical_Column2 are more spread out from the mean compared to Numerical_Column1.")
    else:
        print("\nBoth Numerical_Column1 and Numerical_Column2 have the same standard deviation.")
        print("This means the data points in both datasets have a similar spread around the mean.")
else:
    print("The 'descriptive_stats' DataFrame is not available. Please run the cell to generate descriptive statistics first.")

"""#Q20. Write a Python program to visualize covariance using a heatmap."""

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming 'df' pandas DataFrame is available from previous steps
# If not, you can create or load your DataFrame here with multiple numerical columns.
# Example:
# import pandas as pd
# import numpy as np
# data = {'Numerical_Column1': np.random.rand(100) * 100,
#         'Numerical_Column2': np.random.randint(1, 50, 100),
#         'Numerical_Column3': np.random.normal(0, 1, 100)}
# df = pd.DataFrame(data)

# Select only the numerical columns for calculating covariance
numerical_df = df.select_dtypes(include=np.number)

# Calculate the covariance matrix
covariance_matrix = numerical_df.cov()

# Create a heatmap of the covariance matrix
plt.figure(figsize=(8, 6))
sns.heatmap(covariance_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Covariance Heatmap of Numerical Columns')
plt.show()

"""#Q21.Use seaborn to create a correlation matrix for a dataset."""

import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Assuming 'df' pandas DataFrame is available from previous steps
# If not, you can create or load your DataFrame here with multiple numerical columns.
# Example:
# import pandas as pd
# import numpy as np
# data = {'Numerical_Column1': np.random.rand(100) * 100,
#         'Numerical_Column2': np.random.randint(1, 50, 100),
#         'Numerical_Column3': np.random.normal(0, 1, 100)}
# df = pd.DataFrame(data)


# Select only the numerical columns for calculating correlation
numerical_df = df.select_dtypes(include=np.number)

# Calculate the correlation matrix
correlation_matrix = numerical_df.corr()

# Create a heatmap of the correlation matrix
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Correlation Heatmap of Numerical Columns')
plt.show()

"""#Q22.Generate a dataset and implement both variance and standard deviation computations."""

import numpy as np
import math

# Reuse the functions defined in previous cells
# from your_module import calculate_variance, calculate_standard_deviation

# Generate a sample dataset
dataset = [15, 22, 18, 25, 30, 20, 28, 15, 22, 26]

# Calculate variance
variance = calculate_variance(dataset)

# Calculate standard deviation
standard_deviation = calculate_standard_deviation(dataset)

print(f"Dataset: {dataset}")
print(f"Variance: {variance:.2f}")
print(f"Standard Deviation: {standard_deviation:.2f}")

"""#Q23.Visualize skewness and kurtosis using Python libraries like matplotlib or seaborn."""

import matplotlib.pyplot as plt
import numpy as np
import scipy.stats as stats

# Assuming 'datasets' dictionary is already defined from previous steps
# If not, you can create or load your dataset here.
# Example:
# datasets = {
#     "Positive Skewness": np.random.gamma(2, 2, 1000),
#     "Negative Skewness": -np.random.gamma(2, 2, 1000),
#     "No Skewness (Normal)": np.random.normal(0, 1, 1000)
# }

# Plot histograms for each dataset
plt.figure(figsize=(15, 5))

for i, (name, data) in enumerate(datasets.items()):
    plt.subplot(1, 3, i + 1)
    plt.hist(data, bins=30, edgecolor='black', alpha=0.7)
    plt.title(name)
    plt.xlabel("Value")
    plt.ylabel("Frequency")
    # Optional: Add mean, median, mode lines to highlight skewness
    mean_val = np.mean(data)
    median_val = np.median(data)
    # Mode is less meaningful for continuous data, so we won't plot it here

    plt.axvline(mean_val, color='red', linestyle='dashed', linewidth=1, label=f'Mean: {mean_val:.2f}')
    plt.axvline(median_val, color='green', linestyle='dashed', linewidth=1, label=f'Median: {median_val:.2f}')
    plt.legend()


plt.tight_layout()
plt.show()

# Explain how the histograms relate to skewness and kurtosis
print("Visualizing Skewness and Kurtosis:")
print("\nSkewness is visually represented by the asymmetry of the histogram:")
print("- Positive skewness: The tail is longer on the right side. The mean is typically greater than the median.")
print("- Negative skewness: The tail is longer on the left side. The mean is typically less than the median.")
print("- No skewness (Symmetrical): The histogram is roughly symmetrical. The mean, median, and mode are close.")

print("\nKurtosis relates to the 'tailedness' and peak of the distribution:")
print("- Leptokurtic (Kurtosis > 0 for excess kurtosis, > 3 for normal kurtosis): The distribution has heavier tails and a sharper peak than a normal distribution.")
print("- Platykurtic (Kurtosis < 0 for excess kurtosis, < 3 for normal kurtosis): The distribution has lighter tails and a flatter peak than a normal distribution.")
print("- Mesokurtic (Kurtosis ≈ 0 for excess kurtosis, ≈ 3 for normal kurtosis): The distribution has tails and a peak similar to a normal distribution.")

"""#Q24. Implement the Pearson and Spearman correlation coefficients for a dataset."""

import pandas as pd
import numpy as np

# Assuming 'df' pandas DataFrame is available from previous steps
# If not, you can create or load your DataFrame here with multiple numerical columns.
# Example:
# import pandas as pd
# import numpy as np
# data = {'Numerical_Column1': np.random.rand(100) * 100,
#         'Numerical_Column2': np.random.randint(1, 50, 100),
#         'Numerical_Column3': np.random.normal(0, 1, 100)}
# df = pd.DataFrame(data)

# Select only the numerical columns for calculating correlation
numerical_df = df.select_dtypes(include=np.number)

# Calculate the Pearson correlation matrix
pearson_corr_matrix = numerical_df.corr(method='pearson')

# Calculate the Spearman correlation matrix
spearman_corr_matrix = numerical_df.corr(method='spearman')

print("Pearson Correlation Matrix:")
display(pearson_corr_matrix)

print("\nSpearman Correlation Matrix:")
display(spearman_corr_matrix)